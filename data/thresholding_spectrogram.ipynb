{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import analysis\n",
    "import db\n",
    "import fetcher\n",
    "from recordings import Recording\n",
    "from species import CommonName\n",
    "from trim_recordings import detect_utterances\n",
    "\n",
    "import IPython\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature\n",
    "import matplotlib\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydub\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import collections\n",
    "import hashlib\n",
    "import io\n",
    "import multiprocessing\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=matplotlib.MatplotlibDeprecationWarning)\n",
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = db.create_session('master.db')\n",
    "recordings_fetcher = fetcher.Fetcher('recordings', pool_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load recordings from the database and filter them according to some selection criteria:\n",
    "right species, contains the data we need, good quality, not too short and not too long.\n",
    "\n",
    "Then get them from the cache or download them if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5(string):\n",
    "    m = hashlib.md5()\n",
    "    m.update(string.encode('utf-8'))\n",
    "    return m.digest()\n",
    "\n",
    "species = session.query(CommonName).filter(\n",
    "    CommonName.language_code == 'nl',\n",
    "    CommonName.common_name == 'Tjiftjaf',\n",
    ").one().species\n",
    "genus, species = species.scientific_name.split(' ')\n",
    "\n",
    "recordings = [\n",
    "    r for r in session.query(Recording).filter(Recording.genus == genus, Recording.species == species)\n",
    "    if r.url and r.audio_url and not r.background_species and r.quality == 'A' and 10 <= r.length_seconds <= 120\n",
    "]\n",
    "recordings.sort(key=lambda recording: md5(recording.recording_id))\n",
    "print(f'Found {len(recordings)} candidate recordings')\n",
    "\n",
    "recordings = recordings[:12]\n",
    "#recordings = recordings[12:24]\n",
    "recordings = {r.recording_id: r for r in recordings}\n",
    "\n",
    "def load_recording(recording):\n",
    "    data = recordings_fetcher.fetch_cached(recording.audio_url)\n",
    "    sound = analysis.load_sound(io.BytesIO(data))\n",
    "    return (recording.recording_id, sound)\n",
    "\n",
    "pool = multiprocessing.pool.Pool(8)\n",
    "sounds = dict(tqdm(pool.imap(load_recording, recordings.values(), 1), total=len(recordings)))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze and plot the results, ordered by descending noise volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyses = {}\n",
    "for recording_id, sound in sounds.items():\n",
    "    analyses[recording_id] = analysis.Analysis(sound)\n",
    "\n",
    "for (recording_id, a) in sorted(analyses.items(), key=lambda ia: ia[1].perceptual_noise_volume_db):\n",
    "    # Create figure.\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(9, 3), gridspec_kw={'width_ratios': [8, 1]}, sharey='all')\n",
    "    \n",
    "    # Display spectrogram on the left.\n",
    "    img = librosa.display.specshow(librosa.power_to_db(a.mel_spectrogram),\n",
    "                                   x_axis='time', y_axis='mel', fmin=0.0, fmax=analysis.SAMPLE_RATE / 2,\n",
    "                                   cmap='magma', vmin=-80, vmax=0,\n",
    "                                   hop_length=analysis.FFT_HOP_LENGTH, sr=analysis.SAMPLE_RATE, ax=ax[0])\n",
    "    ax[0].set_title(f'{recording_id} - {recordings[recording_id].type}')\n",
    "    ax[0].title.set_url('https:' + recordings[recording_id].url)\n",
    "    \n",
    "    # Display vocalizations as transparent green rectangles on top of the spectrogram.\n",
    "    for (start, end) in a.vocalizations:\n",
    "        ax[0].add_patch(\n",
    "            Rectangle((start, 0), end - start, analysis.SAMPLE_RATE / 2,\n",
    "                      edgecolor='none', facecolor='#00ff0050'))\n",
    "    \n",
    "    # Plot volume of noise-filtered signal on top of spectrogram.\n",
    "    ax_right = ax[0].twinx()\n",
    "    xs = (np.arange(0, a.mel_spectrogram.shape[1]) + 0.5) * analysis.FFT_HOP_LENGTH / analysis.SAMPLE_RATE\n",
    "    ax_right.plot(xs, a.filtered_volume_db,\n",
    "                  linewidth=1.0)\n",
    "    ax_right.set_ylim(-80, 0)\n",
    "    \n",
    "    # Add lines at our vocalization thresholds.\n",
    "    ax_right.axhline(a.noise_volume_db, color='red')\n",
    "    ax_right.axhline(a.noise_volume_db + analysis.VOCALIZATION_TRIGGER_THRESHOLD_DB, color='yellow')\n",
    "    ax_right.axhline(a.noise_volume_db + analysis.VOCALIZATION_KEEP_THRESHOLD_DB, color='green')\n",
    "    \n",
    "    # Display noise profile on the right.\n",
    "    img = librosa.display.specshow(librosa.power_to_db(a.noise_profile),\n",
    "                                   y_axis='mel', fmin=0.0, fmax=analysis.SAMPLE_RATE / 2, ax=ax[1],\n",
    "                                   cmap='magma', vmin=-80, vmax=0)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_ylabel(None)\n",
    "    ax[1].set_title(f'Noise: {a.perceptual_noise_volume_db:.1f} dB')\n",
    "    fig.colorbar(img, ax=ax[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    IPython.display.display(IPython.display.Audio(sounds[recording_id], rate=analysis.SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a large empty cell to end with, to prevent scrolling when re-evaluating the output of the previous cell.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
