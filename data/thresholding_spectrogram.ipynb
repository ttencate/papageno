{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import db\n",
    "import fetcher\n",
    "from recordings import Recording\n",
    "from trim_recordings import detect_utterances\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import display\n",
    "import librosa\n",
    "import librosa.display\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydub\n",
    "import scipy.ndimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import io\n",
    "import multiprocessing\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = db.create_session('master.db')\n",
    "recordings_fetcher = fetcher.Fetcher('recordings', pool_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recordings from the database and filter them according to some selection criteria:\n",
    "# right species, contains the data we need, good quality, not too short and not too long.\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def md5(string):\n",
    "    m = hashlib.md5()\n",
    "    m.update(string.encode('utf-8'))\n",
    "    return m.digest()\n",
    "\n",
    "recordings = [\n",
    "    r for r in session.query(Recording).filter(\n",
    "        #Recording.genus == 'Turdus', Recording.species == 'merula', # Merel\n",
    "        #Recording.genus == 'Passer', Recording.species == 'domesticus', # Huismus\n",
    "        #Recording.genus == 'Parus', Recording.species == 'major', # Koolmees\n",
    "        #Recording.genus == 'Acrocephalus', Recording.species == 'palustris', # Bosrietzanger\n",
    "        Recording.genus == 'Botaurus', Recording.species == 'stellaris', # Roerdomp\n",
    "    )\n",
    "    if r.url and r.audio_url and not r.background_species and r.quality == 'A' and 10 <= r.length_seconds <= 120\n",
    "]\n",
    "recordings.sort(key=lambda recording: md5(recording.recording_id))\n",
    "print(f'Found {len(recordings)} candidate recordings')\n",
    "#recordings = recordings[:12]\n",
    "recordings = recordings[12:24]\n",
    "recordings = {r.recording_id: r for r in recordings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (cached) and decode the MP3s. \n",
    "\n",
    "sr = 44100\n",
    "\n",
    "def load_recording(recording):\n",
    "    data = recordings_fetcher.fetch_cached(recording.audio_url)\n",
    "    sound = pydub.AudioSegment.from_file(io.BytesIO(data), 'mp3')\\\n",
    "        .set_channels(1)\\\n",
    "        .set_frame_rate(sr)\\\n",
    "        .set_sample_width(2)\n",
    "    return (recording.recording_id, sound)\n",
    "\n",
    "pool = multiprocessing.pool.Pool(8)\n",
    "sounds = dict(tqdm(pool.imap(load_recording, recordings.values(), 1), total=len(recordings)))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kahl, S. (2020). \"Identifying Birds by Sound: Large-scale Acoustic Event Recognition for\n",
    "Avian Activity Monitoring.\" Dissertation. Chemnitz University of Technology, Chemnitz, Germany.\n",
    "https://monarch.qucosa.de/api/qucosa%3A36986/attachment/ATT-0/\n",
    "\n",
    "In section 2.4.3. \"Adaption to avian acoustic monitoring\", page 58, Kahl recommends a window size of 512\n",
    "samples at 48 kHz with an overlap of 50% (256 samples) using a Hann window function.\n",
    "\n",
    "Furthermore, bird vocalizations are almost always between 150 Hz and 15 kHz. We are not currently using this fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 512\n",
    "hop_length = window_size // 2\n",
    "\n",
    "def spectrogram(sound):\n",
    "    y = np.array(sound.get_array_of_samples()) / 0x8000\n",
    "    D = librosa.stft(y, n_fft=window_size, hop_length=hop_length, window='hann')\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    return S_db\n",
    "\n",
    "spectrograms = {}\n",
    "for recording_id, sound in sounds.items():\n",
    "    spectrograms[recording_id] = spectrogram(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_spectrogram(ax, spectrogram):\n",
    "    librosa.display.specshow(spectrogram, x_axis='time', y_axis='linear', ax=ax, sr=sr, hop_length=hop_length)\n",
    "\n",
    "min_utterance_duration_seconds = 0.1\n",
    "min_utterance_separation_seconds = 0.3\n",
    "    \n",
    "def detect_utterances(lower_volume, upper_volume, lower_threshold, upper_threshold):\n",
    "    # Utterances are ranges of consecutive windows \n",
    "    # whose \"lower\" (more smoothed) volumes are all above lower_threshold,\n",
    "    # and at least one of whose \"upper\" (less smoothed) volumes is above upper_threshold.\n",
    "    utterances = []\n",
    "    start = None\n",
    "    reached_upper = False\n",
    "    for i, (lv, uv) in enumerate(zip(lower_volume, upper_volume)):\n",
    "        if lv < lower_threshold:\n",
    "            if start is not None:\n",
    "                if reached_upper:\n",
    "                    utterances.append((start, i))\n",
    "                start = None\n",
    "                reached_upper = False\n",
    "        else:\n",
    "            if start is None:\n",
    "                start = i\n",
    "            if uv >= upper_threshold:\n",
    "                reached_upper = True\n",
    "    if start is not None and reached_upper:\n",
    "        utterances.append((start, len(volume)))\n",
    "    \n",
    "    # Convert to seconds.\n",
    "    utterances = [\n",
    "        (start * hop_length / sr, end * hop_length / sr)\n",
    "        for (start, end) in utterances\n",
    "    ]\n",
    "        \n",
    "    # Merge all utterances that are close to each other.\n",
    "    merged_utterances = []\n",
    "    for i, (start, end) in enumerate(utterances):\n",
    "        if merged_utterances and start <= merged_utterances[-1][1] + min_utterance_separation_seconds:\n",
    "            merged_utterances[-1] = (merged_utterances[-1][0], end)\n",
    "        else:\n",
    "            merged_utterances.append((start, end))\n",
    "    \n",
    "    # Retain only those that are long enough.\n",
    "    return [\n",
    "        (start, end)\n",
    "        for (start, end) in merged_utterances\n",
    "        if end - start >= min_utterance_duration_seconds\n",
    "    ]\n",
    "\n",
    "def show_utterance(ax, utterance):\n",
    "    start, end = utterance\n",
    "    ax.add_patch(Rectangle((start, 0), end - start, sr / 2, edgecolor='none', facecolor='#00ff0040'))\n",
    "\n",
    "lower_kernel_sigma_seconds = 0.3\n",
    "upper_kernel_sigma_seconds = 0.02\n",
    "    \n",
    "for recording_id, spectrogram in spectrograms.items():\n",
    "    volume = np.quantile(spectrogram, q=0.95, axis=0)\n",
    "    lower_volume = scipy.ndimage.gaussian_filter(volume, lower_kernel_sigma_seconds * sr / hop_length)\n",
    "    upper_volume = scipy.ndimage.gaussian_filter(volume, upper_kernel_sigma_seconds * sr / hop_length)\n",
    "    \n",
    "    fade_duration = int(min(len(volume_profile) / 5, 1.0 * sr / hop_length))\n",
    "    noise_floor = np.quantile(lower_volume[fade_duration:-fade_duration], q=0.05)\n",
    "    signal_ceil = np.quantile(upper_volume[fade_duration:-fade_duration], q=0.95)\n",
    "    lower_threshold = 0.7 * noise_floor + 0.3 * signal_ceil\n",
    "    upper_threshold = 0.2 * noise_floor + 0.8 * signal_ceil\n",
    "    utterances = detect_utterances(lower_volume, upper_volume, lower_threshold, upper_threshold)\n",
    "    \n",
    "    fig = plt.figure(figsize=(11, 2))\n",
    "    ax_left = plt.axes()\n",
    "    \n",
    "    show_spectrogram(ax_left, spectrogram)\n",
    "    \n",
    "    for utterance in utterances:\n",
    "        show_utterance(ax_left, utterance)\n",
    "    \n",
    "    ax_right = ax_left.twinx()\n",
    "    ax_right.set_ylim(-80, 0)\n",
    "    ax_right.plot(np.arange(0, spectrogram.shape[1]) * hop_length / sr, volume, color='blue')\n",
    "    ax_right.plot(np.arange(0, spectrogram.shape[1]) * hop_length / sr, lower_volume, color='green')\n",
    "    ax_right.plot(np.arange(0, spectrogram.shape[1]) * hop_length / sr, upper_volume, color='red')\n",
    "    ax_right.axhline(lower_threshold, color='green')\n",
    "    ax_right.axhline(upper_threshold, color='red')\n",
    "    \n",
    "    ax_left.set_title(f'{recording_id} â€” snr: {signal_ceil - noise_floor:.0f} dB')\n",
    "    ax_left.title.set_url('https:' + recordings[recording_id].url)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    display(IPython.display.Audio(sounds[recording_id].get_array_of_samples(), rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
